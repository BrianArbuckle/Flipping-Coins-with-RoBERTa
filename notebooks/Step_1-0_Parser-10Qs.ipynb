{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caring-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Brian Arbuckle, Benu Atri and Paras Jamil\"\n",
    "__version__ = \"CS224u, Final Project 2021 v1\"\n",
    "__updated__ = \"Mar 23, 2021, 11pmCST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "straight-reaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-03-24'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from lxml import etree\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from parse_token_utils import create_directory, parse_fileName_vals\n",
    "from filing_parsers import *\n",
    "\n",
    "today = str(dt.today()).split(' ')[0]\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-official",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>At some point</b><br> We will need to define what the labels are '1's and '0's as the discrepancy between performance_earnings and performance_filing this is true for about 10% - this is related to our hypothesis, the text that we need to show that it changes sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "molecular-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied & adapted from data-engineering/parsers/filing_parsers.py (git)\n",
    "forms = ['10-K']\n",
    "def quick_parse(filepath):\n",
    "    parser = etree.XMLParser(recover=True, huge_tree=True)\n",
    "    tree = etree.parse(filepath, parser)\n",
    "    notags = etree.tostring(tree, encoding='utf8', method='text')\n",
    "    return notags\n",
    "def parse_10q(filepath, cik, year):\n",
    "    \"\"\"\n",
    "    Parses the 10-Q passed. Returns a dictionary:\n",
    "        body: The body of the 10-Q\n",
    "    :param filepath:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    results = {}\n",
    "    with open(filepath) as fp:\n",
    "        soup = BeautifulSoup(fp, \"lxml\")\n",
    "\n",
    "    #results['TenKText'] = __extract_document_text(soup)\n",
    "    results['headers'] = __parse_sec_header(soup, cik, year)\n",
    "    results['documents'] = __parse_documents(soup)\n",
    "    return results\n",
    "\n",
    "def get_date(filingPath):\n",
    "    \"\"\"From FinBert\"\"\"\n",
    "    accession_code = filingPath.split('/')[-2]\n",
    "    ticker = filingPath.split('/')[-4]\n",
    "    filing_type = filingPath.split('/')[-3]\n",
    "    #print(accession_code, ticker, filing_type)\n",
    "    code = accession_code+'.txt'\n",
    "    with open(filingPath, 'r') as f:\n",
    "        readlin = f.read()\n",
    "    for i in readlin.split('\\n'):\n",
    "#         print(i)\n",
    "        if code in i:\n",
    "            date_extr = i.split(' : ')[1]\n",
    "            return date_extr\n",
    "\n",
    "\n",
    "def __parse_documents(soup):\n",
    "    import time\n",
    "    # Get all the douments\n",
    "    result_documents = []\n",
    "    documents = soup.find_all('document')\n",
    "\n",
    "    for doc in documents:\n",
    "\n",
    "        type_node = doc.find('type')\n",
    "        if type_node:\n",
    "            type_text = type_node.contents[0].strip()\n",
    "            desc_node = type_node.find('description')\n",
    "        else:\n",
    "            type_text = 'NA'\n",
    "\n",
    "        seq_node = doc.find('sequence')\n",
    "        if seq_node:\n",
    "            seq_text = seq_node.contents[0].strip()\n",
    "        else:\n",
    "            seq_text = str(int(time.time()))[-6:]\n",
    "\n",
    "        if desc_node:\n",
    "            desc_text = desc_node.contents[0]\n",
    "        else:\n",
    "            desc_text = ''\n",
    "\n",
    "        if type_text in forms:\n",
    "            is_10_q = True\n",
    "        else:\n",
    "            is_10_q = False\n",
    "\n",
    "        if type_text not in [\"XML\", \"GRAPHIC\", \"EXCEL\", \"ZIP\"]:\n",
    "            logging.debug(\"Parsing doucment type: {}\".format(type_text))\n",
    "            result_documents.append(\n",
    "                {'is_10_q': is_10_q,\n",
    "                 'type': type_text,\n",
    "                 'sequence': seq_text,\n",
    "                 'description': desc_text,\n",
    "                 'document': __extract_document_text(soup)}\n",
    "            )\n",
    "\n",
    "    return result_documents\n",
    "\n",
    "def __parse_sec_header(soup, cik, year):\n",
    "    sec_header = soup.find(\"sec-header\")\n",
    "\n",
    "    if not sec_header:\n",
    "        sec_header = soup\n",
    "\n",
    "    result = {\n",
    "        'cik': cik,\n",
    "        'year': year,\n",
    "    }\n",
    "    result['accession_number'] = __get_line_item(sec_header, 'ACCESSION NUMBER:')\n",
    "    result['conformed_period_of_report'] = __get_line_item(sec_header, 'CONFORMED PERIOD OF REPORT:')\n",
    "    result['filed_as_of_date'] = __get_line_item(sec_header, 'FILED AS OF DATE:')\n",
    "    result['company_confirmed_name'] = __get_line_item(sec_header, 'COMPANY CONFORMED NAME:')\n",
    "    result['central_index_key'] = __get_line_item(sec_header, 'CENTRAL INDEX KEY:')\n",
    "    result['standard_industrial_classification'] = __get_line_item(sec_header, 'STANDARD INDUSTRIAL CLASSIFICATION:')\n",
    "    result['state_of_incorporation'] = __get_line_item(sec_header, 'STATE OF INCORPORATION:')\n",
    "    result['fiscal_year_end'] = __get_line_item(sec_header, 'FISCAL QUARTER END:')\n",
    "\n",
    "    return result\n",
    "\n",
    "def __get_line_item(sec_header, attr_name):\n",
    "    find_results = re.findall(attr_name + '(.*?)\\n', str(sec_header))\n",
    "\n",
    "    if find_results:\n",
    "        return find_results[0].strip()\n",
    "    else:\n",
    "        return None\n",
    "def __extract_document_text(soup):\n",
    "    if len(soup) == 1:\n",
    "        TenKtext = soup.find('text').extract().text\n",
    "        TenKtext = parser_custom(TenKtext)\n",
    "    #tables = document.find_all('table')\n",
    "    #for table in tables:\n",
    "    #    table.decompose()\n",
    "\n",
    "    return TenKtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "private-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET = True\n",
    "PICKLES = '../pickle_data/'\n",
    "DATA = 'data'\n",
    "\n",
    "# GLOBAL START AND END:\n",
    "START = '2016-01-01'\n",
    "END = '2021-03-01'\n",
    "\n",
    "doc_folder_name = 'item7_parsed_documents'\n",
    "\n",
    "if FULL_DATASET == True:\n",
    "    stocks_PATH = os.path.join(PICKLES, \"performance_labeled.pickle\")\n",
    "forms = [ '10-K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neutral-start",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../pickle_data/performance_labeled.pickle'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enabling-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df = pd.read_pickle(stocks_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifth-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df.head()\n",
    "#parsed_sec_filings_path = f'../data/{today}_parsed_sec_filings_path/'\n",
    "#create_directory(parsed_sec_filings_path) \n",
    "parsed_sec_filings_path = '../data/2021-03-20_parsed_sec_filings_path/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-measurement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 of 1620 ------------------------------\n",
      "02 of 1620 ------------------------------\n",
      "03 of 1620 ------------------------------\n",
      "04 of 1620 ------------------------------\n",
      "05 of 1620 ------------------------------\n",
      "06 of 1620 ------------------------------\n",
      "07 of 1620 ------------------------------\n",
      "08 of 1620 ------------------------------\n",
      "09 of 1620 ------------------------------\n",
      "10 of 1620 ------------------------------\n",
      "11 of 1620 ------------------------------\n",
      "12 of 1620 ------------------------------\n",
      "13 of 1620 ------------------------------\n",
      "14 of 1620 ------------------------------\n",
      "15 of 1620 ------------------------------\n",
      "16 of 1620 ------------------------------\n",
      "17 of 1620 ------------------------------\n",
      "18 of 1620 ------------------------------\n",
      "19 of 1620 ------------------------------\n",
      "20 of 1620 ------------------------------\n",
      "21 of 1620 ------------------------------\n",
      "22 of 1620 ------------------------------\n",
      "23 of 1620 ------------------------------\n",
      "24 of 1620 ------------------------------\n",
      "25 of 1620 ------------------------------\n",
      "26 of 1620 ------------------------------\n",
      "27 of 1620 ------------------------------\n",
      "28 of 1620 ------------------------------\n",
      "29 of 1620 ------------------------------\n",
      "30 of 1620 ------------------------------\n",
      "31 of 1620 ------------------------------\n",
      "32 of 1620 ------------------------------\n",
      "33 of 1620 ------------------------------\n",
      "34 of 1620 ------------------------------\n",
      "Parsing a 10-K for AMAT, quarter Oct2018, dated 2018-11-15 00:00:00...\n",
      "6 (6, 2) RangeIndex(start=0, stop=2, step=1)\n",
      "6 (6, 2) Index(['SearchTerm', 'Start'], dtype='object')\n",
      "Bad command or file name ../data/sec-edgar-filings/AMAT/10-K/0000006951-18-000041/full-submission.txt\n",
      "35 of 1620 ------------------------------\n",
      "Parsing a 10-K for AMAT, quarter Oct2017, dated 2017-11-16 00:00:00...\n",
      "6 (6, 2) RangeIndex(start=0, stop=2, step=1)\n",
      "6 (6, 2) Index(['SearchTerm', 'Start'], dtype='object')\n",
      "Bad command or file name ../data/sec-edgar-filings/AMAT/10-K/0000006951-17-000038/full-submission.txt\n",
      "36 of 1620 ------------------------------\n",
      "Parsing a 10-K for AMAT, quarter Oct2016, dated 2016-11-17 00:00:00...\n",
      "6 (6, 2) RangeIndex(start=0, stop=2, step=1)\n",
      "6 (6, 2) Index(['SearchTerm', 'Start'], dtype='object')\n",
      "Bad command or file name ../data/sec-edgar-filings/AMAT/10-K/0000006951-16-000068/full-submission.txt\n",
      "37 of 1620 ------------------------------\n",
      "38 of 1620 ------------------------------\n",
      "39 of 1620 ------------------------------\n",
      "40 of 1620 ------------------------------\n",
      "41 of 1620 ------------------------------\n",
      "42 of 1620 ------------------------------\n",
      "43 of 1620 ------------------------------\n",
      "44 of 1620 ------------------------------\n",
      "45 of 1620 ------------------------------\n",
      "46 of 1620 ------------------------------\n",
      "47 of 1620 ------------------------------\n",
      "48 of 1620 ------------------------------\n",
      "49 of 1620 ------------------------------\n",
      "50 of 1620 ------------------------------\n",
      "51 of 1620 ------------------------------\n",
      "52 of 1620 ------------------------------\n",
      "53 of 1620 ------------------------------\n",
      "54 of 1620 ------------------------------\n",
      "55 of 1620 ------------------------------\n",
      "56 of 1620 ------------------------------\n",
      "57 of 1620 ------------------------------\n",
      "58 of 1620 ------------------------------\n",
      "59 of 1620 ------------------------------\n",
      "60 of 1620 ------------------------------\n",
      "61 of 1620 ------------------------------\n",
      "62 of 1620 ------------------------------\n",
      "63 of 1620 ------------------------------\n",
      "64 of 1620 ------------------------------\n",
      "65 of 1620 ------------------------------\n",
      "66 of 1620 ------------------------------\n",
      "67 of 1620 ------------------------------\n",
      "68 of 1620 ------------------------------\n",
      "69 of 1620 ------------------------------\n",
      "70 of 1620 ------------------------------\n",
      "71 of 1620 ------------------------------\n",
      "72 of 1620 ------------------------------\n",
      "73 of 1620 ------------------------------\n",
      "74 of 1620 ------------------------------\n",
      "75 of 1620 ------------------------------\n",
      "76 of 1620 ------------------------------\n",
      "77 of 1620 ------------------------------\n",
      "78 of 1620 ------------------------------\n",
      "79 of 1620 ------------------------------\n",
      "80 of 1620 ------------------------------\n",
      "81 of 1620 ------------------------------\n",
      "82 of 1620 ------------------------------\n",
      "Parsing a 10-K for CMCSA, quarter Dec2019, dated 2020-01-23 00:00:00...\n"
     ]
    }
   ],
   "source": [
    "# uncomment below - if we needed to re-download, \n",
    "\n",
    "forms = ['10-K']\n",
    "from bs4 import BeautifulSoup\n",
    "#create_directory(os.path.join(sec_filings_path,doc_folder_name)) #if exists, will not create again\n",
    "sec_filings_path = '../data/sec-edgar-filings/'\n",
    "def run_parse(stocks_df):\n",
    "    num_stocks = stocks_df.shape[0]\n",
    "    c = 0\n",
    "\n",
    "    for idx, row in stocks_df.iterrows():\n",
    "        if row['form'] in forms:\n",
    "                \n",
    "            ticker_form_path = os.path.join(sec_filings_path, row['ticker'], row['form'][:4])\n",
    "            ciks = os.listdir(ticker_form_path)\n",
    "            #print(ticker_form_path)\n",
    "            for cik in ciks:  \n",
    "                #print(cik)\n",
    "                if not cik == '.DS_Store':\n",
    "                    text_to_parse = os.path.join(ticker_form_path, cik, 'full-submission.txt')\n",
    "                    if row['date_filed'] == dt.strptime(get_date(text_to_parse), '%Y%m%d'):\n",
    "                        c += 1\n",
    "                        print(\"{:0>2d} of {} {}\".format(c, num_stocks, (30*'-')))\n",
    "                        ticker, cik_10dig, filing_type, filing_qrtr, doc_reported_on, doc_year = parse_fileName_vals(cik, row, text_to_parse)\n",
    "                        #input()\n",
    "                        \n",
    "                        out_file_name = ticker+'_'+cik_10dig+'_'+filing_type+'_'+filing_qrtr+ \".txt\"\n",
    "                        out_file_path = os.path.join(parsed_sec_filings_path, out_file_name)\n",
    "                        \n",
    "                        if os.path.exists(out_file_path):\n",
    "                            continue\n",
    "                            \n",
    "                        print(f'Parsing a {filing_type} for {ticker}, quarter {filing_qrtr}, dated {doc_reported_on}...')\n",
    "                        #results = parse_10q(text_to_parse, cik, doc_year)\n",
    "                        \n",
    "    \n",
    "                        with open(text_to_parse) as fp:\n",
    "                            soup = BeautifulSoup(fp, \"lxml\")\n",
    "                        tenK = __extract_document_text(soup)\n",
    "                        if tenK:\n",
    "                            print(len(tenK))\n",
    "                            #print(len(results))\n",
    "                            with open(out_file_path, 'w') as f:\n",
    "                                f.write(tenK)\n",
    "                                print()\n",
    "                            print(f'Parsing finished, output saved in:\\n{out_file_path}')\n",
    "                            print('__'*30)\n",
    "                            print()\n",
    "                                                    \n",
    "                        else:\n",
    "                            print('Bad command or file name', text_to_parse)                        \n",
    "run_parse(stocks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "horizontal-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "discards = []\n",
    "def parser_custom(TenKtext):\n",
    "    \n",
    "    matches = re.compile(r'(item\\s(7[\\.\\s]|8[\\.\\s])|'\n",
    "                         'discussion\\sand\\sanalysis\\sof\\s(consolidated\\sfinancial|financial)\\scondition|'\n",
    "                         '(consolidated\\sfinancial|financial)\\sstatements\\sand\\ssupplementary\\sdata)', re.IGNORECASE)\n",
    "\n",
    "    matches_array = pd.DataFrame([(match.group(), match.start()) for match in matches.finditer(TenKtext)])\n",
    "    print(len(matches_array), matches_array.shape, matches_array.columns)\n",
    "    # Set columns in the dataframe\n",
    "    matches_array.columns = ['SearchTerm', 'Start']\n",
    "    print('hi', len(matches_array), matches_array.shape, matches_array.columns)\n",
    "    # Get the number of rows in the dataframe\n",
    "    Rows = matches_array['SearchTerm'].count()\n",
    "    \n",
    "    # Create a new column in 'matches_array' called 'Selection' and add adjacent 'SearchTerm' (i and i+1 rows) text concatenated\n",
    "    count = 0 # Counter to help with row location and iteration\n",
    "    while count < (Rows-1): # Can only iterate to the second last row\n",
    "        matches_array.at[count,'Selection'] = (matches_array.iloc[count,0] + matches_array.iloc[count+1,0]).lower() # Convert to lower case\n",
    "        count += 1\n",
    "\n",
    "    # Set up 'Item 7/8 Search Pattern' regex patterns\n",
    "    matches_item7 = re.compile(r'(item\\s7\\.discussion\\s[a-z]*)')\n",
    "    matches_item8 = re.compile(r'(item\\s8\\.(consolidated\\sfinancial|financial)\\s[a-z]*)')\n",
    "\n",
    "    # Lists to store the locations of Item 7/8 Search Pattern matches\n",
    "    Start_Loc = []\n",
    "    End_Loc = []\n",
    "\n",
    "    # Find and store the locations of Item 7/8 Search Pattern matches\n",
    "    count = 0 # Set up counter\n",
    "\n",
    "    while count < (Rows-1): # Can only iterate to the second last row\n",
    "\n",
    "        # Match Item 7 Search Pattern\n",
    "        if re.match(matches_item7, matches_array.at[count,'Selection']):\n",
    "            # Column 1 = 'Start' column in 'matches_array'\n",
    "            \n",
    "            Start_Loc.append(matches_array.iloc[count,1]) # Store in list => Item 7 will be the starting location (column '1' = 'Start' column)\n",
    "\n",
    "        # Match Item 8 Search Pattern\n",
    "        if re.match(matches_item8, matches_array.at[count,'Selection']):\n",
    "            End_Loc.append(matches_array.iloc[count,1])\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    print(Start_Loc, End_Loc)\n",
    "    input()\n",
    "    if len(Start_Loc)==len(End_Loc)==0:\n",
    "        return False\n",
    "    if len(Start_Loc)==len(End_Loc)==1:\n",
    "        print(Start_Loc, End_Loc)\n",
    "    \n",
    "\n",
    "        # Extract section of text and store in 'TenKItem7'\n",
    "        TenKItem7 = TenKtext[Start_Loc[0]:End_Loc[0]]\n",
    "        #print(len(TenKItem7))\n",
    "    elif len(Start_Loc)!=len(End_Loc):\n",
    "        TenKItem7 = TenKtext[Start_Loc[0]:End_Loc[-1]]\n",
    "    else :\n",
    "        print(Start_Loc, End_Loc)\n",
    "        # Extract section of text and store in 'TenKItem7'\n",
    "        TenKItem7 = TenKtext[Start_Loc[1]:End_Loc[1]]\n",
    "        #print(len(TenKItem7))\n",
    "        \n",
    "    # Clean newly extracted text\n",
    "    TenKItem7 = TenKItem7.strip() # Remove starting/ending white spaces\n",
    "    TenKItem7 = TenKItem7.replace('\\n', ' ') # Replace \\n (new line) with space\n",
    "    TenKItem7 = TenKItem7.replace('\\r', '') # Replace \\r (carriage returns-if you're on windows) with space\n",
    "    TenKItem7 = TenKItem7.replace(' ', ' ') # Replace \" \" (a special character for space in HTML) with space\n",
    "    TenKItem7 = TenKItem7.replace(' ', ' ') # Replace \" \" (a special character for space in HTML) with space\n",
    "\n",
    "    while '  ' in TenKItem7:\n",
    "        TenKItem7 = TenKItem7.replace('  ', ' ') # Remove extra spaces\n",
    "\n",
    "    # Print first 500 characters of newly extracted text\n",
    "    #print(TenKItem7[:500])\n",
    "    TenKItem7 = TenKItem7.replace(u'/xa0', u' ') # \n",
    "    return TenKItem7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "remarkable-nicholas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'> 1\n",
      "6 (6, 2) RangeIndex(start=0, stop=2, step=1)\n",
      "hi 6 (6, 2) Index(['SearchTerm', 'Start'], dtype='object')\n",
      "[] []\n",
      "\n",
      "nope\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d2700b3bc4c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mTenKtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "badfile = '../data/sec-edgar-filings/AMAT/10-K/0000006951-18-000041/full-submission.txt'\n",
    "badfile_html = '../data/sec-edgar-filings/AMAT/10-K/0000006951-18-000041/filing-details.html'\n",
    "badfile = '../data/sec-edgar-filings/AMAT/10-K/0000006951-16-000068/full-submission.txt'\n",
    "with open(badfile) as fp:\n",
    "    #soup = BeautifulSoup(fp, \"lxml-xml\")\n",
    "    soup = BeautifulSoup(fp, \"html.parser\")\n",
    "    print(type(soup), len(soup))\n",
    "    tenK = __extract_document_text(soup)\n",
    "    #tenK = filing_document.find('text').extract().text\n",
    "    #tenK = soup.find('text').extract().text\n",
    "    #                        with open(text_to_parse) as fp:\n",
    "    #                        soup = BeautifulSoup(fp, \"lxml\")\n",
    "     \n",
    "\n",
    "\n",
    "    if tenK:\n",
    "        print(len(tenK))\n",
    "        TenKtext = parser_custom(tenK)\n",
    "    else:\n",
    "        print('nope')\n",
    "    \n",
    "\n",
    "TenKtext[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-shopping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-yield",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
